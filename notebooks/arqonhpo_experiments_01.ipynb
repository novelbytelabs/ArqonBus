{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf19e5a",
   "metadata": {},
   "source": [
    "<!-- Cell 1 -->\n",
    "\n",
    "# ArqonHPO Experiments 01 — Winner Stack on Real Data\n",
    "\n",
    "This notebook packages the retained ArqonHPO winners (Exp4,6,8,15,18,25,27) and benchmarks them against a plain random-search baseline on two sklearn datasets. Run this in an environment with `scikit-learn`, `numpy`, and optionally `pandas`/`matplotlib` (e.g., your `helios-gpu-118` env).\n",
    "\n",
    "**Winner stack included**\n",
    "- Exp6 coarse-to-fine C refinement\n",
    "- Exp8 seed coevolution (multi-seed CV)\n",
    "- Exp15 μ+λ evolution\n",
    "- Exp18 explore/exploit cadence\n",
    "- Exp25 sampler hyper co-evo (logC prior mean/std)\n",
    "- Exp27 evolution factor ramp\n",
    "- Exp4 pheromone bandit (penalty weighting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a668e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, fetch_openml\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01142a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 not available (skip): Dataset not found or corrupted. You can use download=True to download it\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "\n",
    "# Real + synthetic datasets (no network by default; MNIST and CIFAR10 added if available)\n",
    "cls = load_breast_cancer()\n",
    "reg = load_diabetes()\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# Synthetic structured tasks\n",
    "\n",
    "def make_structured_cls(n_samples=800, n_features=12, signal=3.0, noise=0.3):\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    w = rng.randn(n_features)\n",
    "    z = X @ w * signal + rng.normal(0, noise, size=n_samples)\n",
    "    p = 1.0 / (1.0 + np.exp(-z))\n",
    "    y = (rng.rand(n_samples) < p).astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_structured_reg(n_samples=800, n_features=10, signal=4.0, noise=0.4):\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    w = rng.randn(n_features)\n",
    "    y = X @ w * signal + rng.normal(0, noise, size=n_samples)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_struct_cls, y_struct_cls = make_structured_cls()\n",
    "X_chaos_cls, y_chaos_cls = X_struct_cls.copy(), rng.permutation(y_struct_cls)\n",
    "X_struct_reg, y_struct_reg = make_structured_reg()\n",
    "\n",
    "# Optional MNIST (requires network or local cache). If unavailable, it is skipped.\n",
    "mnist_entry = None\n",
    "try:\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X_mnist = mnist.data.astype(np.float32) / 255.0\n",
    "    y_mnist = mnist.target.astype(int)\n",
    "    mnist_entry = {\n",
    "        'task': 'cls',\n",
    "        'X': X_mnist,\n",
    "        'y': y_mnist,\n",
    "    }\n",
    "except Exception as e:\n",
    "    print('MNIST not available (skip):', e)\n",
    "\n",
    "# Optional CIFAR-10 via torchvision (requires torch/torchvision and local cache; no download attempted).\n",
    "cifar_entry = None\n",
    "try:\n",
    "    import torchvision.transforms as T  # type: ignore\n",
    "    from torchvision.datasets import CIFAR10  # type: ignore\n",
    "\n",
    "    cifar = CIFAR10(root=str(Path('~/.cache/cifar10').expanduser()), train=True, download=False, transform=T.ToTensor())\n",
    "    n_subset = min(8000, len(cifar))\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(n_subset):\n",
    "        img, label = cifar[i]\n",
    "        X_list.append(img.view(-1).numpy().astype(np.float32))\n",
    "        y_list.append(label)\n",
    "    X_cifar = np.stack(X_list)\n",
    "    y_cifar = np.array(y_list)\n",
    "    cifar_entry = {\n",
    "        'task': 'cls',\n",
    "        'X': X_cifar,\n",
    "        'y': y_cifar,\n",
    "    }\n",
    "except Exception as e:\n",
    "    print('CIFAR10 not available (skip):', e)\n",
    "\n",
    "# Registry mixes real and synthetic tasks to show strength on structured vs chaotic signals.\n",
    "# Lift expected on structured_cls/reg; chaos_cls acts as a stress test.\n",
    "dataset_registry = {\n",
    "    'breast_cancer_cls': {\n",
    "        'task': 'cls',\n",
    "        'X': cls.data,\n",
    "        'y': cls.target,\n",
    "    },\n",
    "    'diabetes_reg': {\n",
    "        'task': 'reg',\n",
    "        'X': reg.data,\n",
    "        'y': reg.target,\n",
    "    },\n",
    "    'structured_cls': {\n",
    "        'task': 'cls',\n",
    "        'X': X_struct_cls,\n",
    "        'y': y_struct_cls,\n",
    "    },\n",
    "    'chaos_cls': {\n",
    "        'task': 'cls',\n",
    "        'X': X_chaos_cls,\n",
    "        'y': y_chaos_cls,\n",
    "    },\n",
    "    'structured_reg': {\n",
    "        'task': 'reg',\n",
    "        'X': X_struct_reg,\n",
    "        'y': y_struct_reg,\n",
    "    },\n",
    "}\n",
    "\n",
    "if mnist_entry is not None:\n",
    "    dataset_registry['mnist_cls'] = mnist_entry\n",
    "if cifar_entry is not None:\n",
    "    dataset_registry['cifar10_cls'] = cifar_entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e970b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    params: Dict[str, float]\n",
    "    metric: float\n",
    "\n",
    "\n",
    "def evaluate_model(task: str, params: Dict[str, float], X, y, seeds=(0, 1)) -> float:\n",
    "    \"\"\"Return a loss metric (lower is better). Uses two seeds for seed coevo (Exp8).\"\"\"\n",
    "    scores: List[float] = []\n",
    "    for seed in seeds:\n",
    "        if task == \"cls\":\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "            model = LogisticRegression(\n",
    "                penalty=params[\"penalty\"],\n",
    "                C=params[\"C\"],\n",
    "                solver=\"liblinear\",\n",
    "                max_iter=800,\n",
    "            )\n",
    "            loss = -float(cross_val_score(model, X, y, cv=cv, scoring=\"neg_log_loss\").mean())\n",
    "        else:\n",
    "            cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "            model = ElasticNet(\n",
    "                alpha=params[\"alpha\"],\n",
    "                l1_ratio=params[\"l1_ratio\"],\n",
    "                random_state=seed,\n",
    "                max_iter=6000,\n",
    "            )\n",
    "            rmse = math.sqrt(\n",
    "                -float(cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\").mean())\n",
    "            )\n",
    "            loss = rmse\n",
    "        scores.append(loss)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def baseline_random_search(task: str, X, y, n_trials: int = 50, rng_seed: int = 13) -> Tuple[List[float], List[Dict[str, float]]]:\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    history: List[float] = []\n",
    "    params_list: List[Dict[str, float]] = []\n",
    "    best = float(\"inf\")\n",
    "    for _ in range(n_trials):\n",
    "        if task == \"cls\":\n",
    "            logC = rng.uniform(-2.5, 2.5)\n",
    "            params = {\"penalty\": rng.choice([\"l1\", \"l2\"]), \"C\": 10 ** logC}\n",
    "        else:\n",
    "            log_alpha = rng.uniform(-3.0, 1.0)\n",
    "            params = {\"alpha\": 10 ** log_alpha, \"l1_ratio\": rng.uniform(0.05, 0.95)}\n",
    "        loss = evaluate_model(task, params, X, y)\n",
    "        best = min(best, loss)\n",
    "        history.append(best)\n",
    "        params_list.append(params)\n",
    "    return history, params_list\n",
    "\n",
    "\n",
    "def winner_stack(task: str, X, y, total_evals: int = 52, rng_seed: int = 7) -> Tuple[List[float], List[Dict[str, float]]]:\n",
    "    \"\"\"Implements winners: Exp6,8,15,18,25,27 (+Exp4) in one loop.\"\"\"\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    history: List[float] = []\n",
    "    params_list: List[Dict[str, float]] = []\n",
    "\n",
    "    if task == \"cls\":\n",
    "        logC_grid = np.linspace(-2.2, 2.0, 6)\n",
    "        base_penalties = [\"l1\", \"l2\"]\n",
    "    else:\n",
    "        logC_grid = np.linspace(-3.0, 1.0, 6)  # for alpha\n",
    "        base_penalties = [\"elastic\"]\n",
    "\n",
    "    # --- Exp6 coarse-to-fine seeds\n",
    "    population: List[EvalResult] = []\n",
    "    for logc in logC_grid:\n",
    "        for pen in base_penalties:\n",
    "            if task == \"cls\":\n",
    "                params = {\"penalty\": pen, \"C\": 10 ** logc}\n",
    "            else:\n",
    "                params = {\"alpha\": 10 ** logc, \"l1_ratio\": rng.uniform(0.05, 0.95)}\n",
    "            loss = evaluate_model(task, params, X, y)\n",
    "            population.append(EvalResult(params=params, metric=loss))\n",
    "            params_list.append(params)\n",
    "            history.append(min([p.metric for p in population]))\n",
    "    population.sort(key=lambda x: x.metric)\n",
    "\n",
    "    mu = min(6, len(population))  # parents\n",
    "    lam = 8  # offspring per generation\n",
    "    gens = max(1, (total_evals - len(population)) // lam)\n",
    "\n",
    "    # Initialize sampler hyper params (Exp25)\n",
    "    if task == \"cls\":\n",
    "        logC_mu = np.mean([math.log10(p.params[\"C\"]) for p in population[:mu]])\n",
    "        logC_std = np.std([math.log10(p.params[\"C\"]) for p in population[:mu]]) + 0.2\n",
    "    else:\n",
    "        logC_mu = np.mean([math.log10(p.params[\"alpha\"]) for p in population[:mu]])\n",
    "        logC_std = np.std([math.log10(p.params[\"alpha\"]) for p in population[:mu]]) + 0.2\n",
    "\n",
    "    pheromone = {pen: 1.0 for pen in base_penalties}  # Exp4\n",
    "\n",
    "    eval_count = len(population)\n",
    "    while eval_count < total_evals:\n",
    "        # Update pheromone based on current top-k\n",
    "        topk = population[:mu]\n",
    "        for pen in base_penalties:\n",
    "            hits = sum(1 for p in topk if p.params.get(\"penalty\", pen) == pen)\n",
    "            pheromone[pen] = 0.2 + 0.8 * (hits / max(1, len(topk)))\n",
    "\n",
    "        # Sampler hyper co-evo update (Exp25)\n",
    "        if task == \"cls\":\n",
    "            logC_mu = 0.7 * logC_mu + 0.3 * np.mean([math.log10(p.params[\"C\"]) for p in topk])\n",
    "            logC_std = 0.6 * logC_std + 0.4 * (np.std([math.log10(p.params[\"C\"]) for p in topk]) + 0.15)\n",
    "        else:\n",
    "            logC_mu = 0.7 * logC_mu + 0.3 * np.mean([math.log10(p.params[\"alpha\"]) for p in topk])\n",
    "            logC_std = 0.6 * logC_std + 0.4 * (np.std([math.log10(p.params[\"alpha\"]) for p in topk]) + 0.15)\n",
    "\n",
    "        # Evolution factor ramp (Exp27) + explore/exploit cadence (Exp18)\n",
    "        gen_idx = max(0, eval_count - len(logC_grid) * len(base_penalties)) // lam\n",
    "        ramp = max(0.25, 1.2 * (1.0 - gen_idx / max(1, gens)))\n",
    "        wide_step = (gen_idx % 4 == 0)\n",
    "        sigma = (0.6 if wide_step else 0.3) * ramp * max(0.2, logC_std)\n",
    "\n",
    "        offspring: List[EvalResult] = []\n",
    "        for _ in range(lam):\n",
    "            parent = rng.choice(topk)\n",
    "            if task == \"cls\":\n",
    "                logc = math.log10(parent.params[\"C\"]) + rng.normal(0, sigma)\n",
    "                logc = float(np.clip(logc, -3.0, 3.0))\n",
    "                penalty = rng.choice(base_penalties, p=np.array([pheromone[p] for p in base_penalties]) / sum(pheromone.values()))\n",
    "                params = {\"penalty\": penalty, \"C\": 10 ** logc}\n",
    "            else:\n",
    "                logc = math.log10(parent.params[\"alpha\"]) + rng.normal(0, sigma)\n",
    "                logc = float(np.clip(logc, -4.0, 2.0))\n",
    "                params = {\"alpha\": 10 ** logc, \"l1_ratio\": float(np.clip(parent.params[\"l1_ratio\"] + rng.normal(0, 0.08), 0.01, 0.99))}\n",
    "\n",
    "            loss = evaluate_model(task, params, X, y)\n",
    "            offspring.append(EvalResult(params=params, metric=loss))\n",
    "            params_list.append(params)\n",
    "            eval_count += 1\n",
    "            best_so_far = min([p.metric for p in population] + [o.metric for o in offspring])\n",
    "            history.append(best_so_far)\n",
    "            if eval_count >= total_evals:\n",
    "                break\n",
    "\n",
    "        population = sorted(population + offspring, key=lambda x: x.metric)[: max(mu, 10)]\n",
    "\n",
    "    population.sort(key=lambda x: x.metric)\n",
    "    return history, [p.params for p in population]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9703402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "results = []\n",
    "all_histories = {}\n",
    "for name, entry in dataset_registry.items():\n",
    "    task, X, y = entry[\"task\"], entry[\"X\"], entry[\"y\"]\n",
    "    base_hist, base_params = baseline_random_search(task, X, y, n_trials=52)\n",
    "    win_hist, win_params = winner_stack(task, X, y, total_evals=52)\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"baseline_best\": base_hist[-1],\n",
    "        \"winner_best\": win_hist[-1],\n",
    "        \"lift\": base_hist[-1] - win_hist[-1],\n",
    "    })\n",
    "    all_histories[name] = {\"baseline\": base_hist, \"winner\": win_hist}\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(summary_df)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ac967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "fig, axes = plt.subplots(1, len(dataset_registry), figsize=(14, 4))\n",
    "if len(dataset_registry) == 1:\n",
    "    axes = [axes]\n",
    "for ax, (name, curves) in zip(axes, all_histories.items()):\n",
    "    ax.plot(curves[\"baseline\"], label=\"Baseline Random Search\", color=\"#999\")\n",
    "    ax.plot(curves[\"winner\"], label=\"Winner Stack\", color=\"#2c7fb8\")\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Evaluations\")\n",
    "    ax.set_ylabel(\"Best loss so far\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564636e3",
   "metadata": {},
   "source": [
    "<!-- Cell 7 -->\n",
    "\n",
    "## Next steps\n",
    "- Run this in `helios-gpu-118` (or any env with scikit-learn/pandas/matplotlib) to generate results.\n",
    "- Swap in your real tasks/datasets; the search wrapper is self-contained.\n",
    "- Adjust `total_evals` and seeds for stability checks or heavier sweeps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helios-gpu-118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
